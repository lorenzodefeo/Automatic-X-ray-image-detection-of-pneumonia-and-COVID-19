{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Allenamento in Google Colab di CustomResNet50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_F9Dxlkpiz95",
    "outputId": "281d4fd5-12bc-47d2-c9c4-a910424f9acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNxOumNi0tej"
   },
   "outputs": [],
   "source": [
    "PATH_FOLDER = '/content/drive/MyDrive/Colab Notebooks/Pneumonia Detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hx9vwQ4p2h1F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "# Parametri dell'allenamento\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0002\n",
    "EPOCHS = 10\n",
    "OPTIMIZER = 'Adam'\n",
    "LOSS_FUNCTION = 'Categorical Cross-Entropy'\n",
    "H, W = 224, 244\n",
    "MODEL = 'CustomResNet50'\n",
    "\n",
    "# Transformation for Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.9, 1.1)),\n",
    "    transforms.Resize((H, W)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: add_gaussian_noise(x) if torch.rand(1).item() > 0.5 else x),\n",
    "])\n",
    "\n",
    "# Dataset e DataLoader\n",
    "path_to_train = os.path.join(PATH_FOLDER, \"Data\", \"train\")\n",
    "path_to_test = os.path.join(PATH_FOLDER, \"Data\", \"test\")\n",
    "train_dataset = datasets.ImageFolder(path_to_train, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(path_to_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CustomResNet50, self).__init__()\n",
    "        # Carica ResNet-50 pre-addestrato su ImageNet\n",
    "        self.base_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.name = 'CustomResNet50'\n",
    "\n",
    "        # Congela i primi 50 layer (tutti tranne gli ultimi blocchi)\n",
    "        layers_to_freeze = 50\n",
    "        for i, child in enumerate(self.base_model.children()):\n",
    "            if i < layers_to_freeze:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Modifica della parte finale della rete\n",
    "        num_features = self.base_model.fc.in_features  # Dimensione dell'output dell'ultimo layer convoluzionale\n",
    "\n",
    "        # Definisci il nuovo classificatore\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),  # Primo Fully Connected\n",
    "            nn.ReLU(),  # Funzione di attivazione\n",
    "            nn.Dropout(0.5),  # Dropout per ridurre l'overfitting\n",
    "            nn.Linear(512, num_classes),  # Strato Fully Connected finale\n",
    "            nn.Softmax(dim=1)  # Strato softmax per probabilitÃ \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "\n",
    "# Funzione per aggiungere rumore gaussiano\n",
    "def add_gaussian_noise(img, mean=0, std=0.25):\n",
    "    noise = torch.randn_like(img) * std + mean\n",
    "    noisy_img = img + noise\n",
    "    noisy_img = torch.clamp(noisy_img, 0., 1.)  # Assicuriamoci che i valori siano tra 0 e 1\n",
    "    return noisy_img\n",
    "\n",
    "\n",
    "# Funzione di allenamento\n",
    "def train_net(model, train_loader, optimizer, criterion, epochs, save_dir='./training_results'):\n",
    "    # Creazione della cartella per salvare i modelli e grafici\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['accuracy'].append(epoch_acc)\n",
    "\n",
    "        # Stampa e salvataggio ogni epoca\n",
    "        message = f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\"\n",
    "        print(message)\n",
    "\n",
    "        # Salvataggio del messaggio nel file di log\n",
    "        log_file_path = os.path.join(save_dir, f\"{model.name}_training_log.txt\")\n",
    "        save_log_to_file(log_file_path, message)\n",
    "\n",
    "        # Salvataggio del modello ad ogni epoca\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f\"{model.name}_model_epoch_{epoch + 1}.pth\"))\n",
    "\n",
    "        # Salvataggio dei grafici alla fine di ogni epoca\n",
    "        plot_training_curves(history, save_dir, epoch + 1)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def save_log_to_file(file_path, message):\n",
    "    \"\"\"\n",
    "    Salva un messaggio in un file di testo. Crea il file se non esiste.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Il percorso del file in cui salvare il messaggio.\n",
    "        message (str): Il messaggio da salvare.\n",
    "    \"\"\"\n",
    "    # Verifica se il file esiste, altrimenti crealo\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as file:  # 'w' crea il file vuoto\n",
    "            pass  # File creato vuoto, pronto per l'aggiunta di messaggi\n",
    "\n",
    "    # Aggiunge il messaggio al file\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(message + '\\n')\n",
    "\n",
    "\n",
    "# Funzione per i grafici\n",
    "def plot_training_curves(history, save_dir, epoch, name = MODEL):\n",
    "    epochs = range(1, len(history['loss']) + 1)  # Epoche da 1 a N\n",
    "\n",
    "    # Creazione dei grafici\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Diagramma della perdita\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['loss'], label='Training Loss', color='red', marker='o')\n",
    "    plt.title('Loss Diagram')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Diagramma dell'accuratezza\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['accuracy'], label='Training Accuracy', color='blue', marker='o')\n",
    "    plt.title('Accuracy Diagram')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Salvataggio dei grafici\n",
    "    plt.savefig(os.path.join(save_dir, f\"{name}_training_curves_epoch_{epoch}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpHDWu1Qyf3V",
    "outputId": "f449fe15-fe93-4189-f5fd-7dacb53819e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7160, Accuracy: 0.7234\n",
      "Epoch 2/100, Loss: 0.4571, Accuracy: 0.8542\n",
      "Epoch 3/100, Loss: 0.3550, Accuracy: 0.8871\n",
      "Epoch 4/100, Loss: 0.3108, Accuracy: 0.8946\n",
      "Epoch 5/100, Loss: 0.2894, Accuracy: 0.9014\n",
      "Epoch 6/100, Loss: 0.2694, Accuracy: 0.9114\n",
      "Epoch 7/100, Loss: 0.2468, Accuracy: 0.9121\n",
      "Epoch 8/100, Loss: 0.2274, Accuracy: 0.9182\n",
      "Epoch 9/100, Loss: 0.2175, Accuracy: 0.9230\n",
      "Epoch 10/100, Loss: 0.2114, Accuracy: 0.9285\n",
      "Epoch 11/100, Loss: 0.2006, Accuracy: 0.9304\n",
      "Epoch 12/100, Loss: 0.1952, Accuracy: 0.9318\n",
      "Epoch 13/100, Loss: 0.2023, Accuracy: 0.9347\n",
      "Epoch 14/100, Loss: 0.1862, Accuracy: 0.9360\n",
      "Epoch 15/100, Loss: 0.1866, Accuracy: 0.9360\n",
      "Epoch 16/100, Loss: 0.1817, Accuracy: 0.9357\n",
      "Epoch 17/100, Loss: 0.1774, Accuracy: 0.9392\n"
     ]
    }
   ],
   "source": [
    "# Definizione dell'ottimizzatore e della loss function\n",
    "model = CustomResNet50()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Allenamento della rete\n",
    "train_net(model, train_loader, optimizer, criterion, epochs=EPOCHS, save_dir=os.path.join(PATH_FOLDER, f'{MODEL}_training_results'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MmRuEG7WgzR"
   },
   "source": [
    "# Applicazione tecniche di OverSampling o DownSampling, per dati sbilanciati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWJUjHDJWrDv",
    "outputId": "bca2dee7-122d-4a49-acb4-63d0d6fcbec8"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione originale delle classi: {0: 460, 1: 1266, 2: 3418}\n",
      "Applicazione del downsampling...\n",
      "Epoch 1/100, Loss: 0.7785, Accuracy: 0.6377\n",
      "Epoch 2/100, Loss: 0.6238, Accuracy: 0.7616\n",
      "Epoch 3/100, Loss: 0.5129, Accuracy: 0.8065\n",
      "Epoch 4/100, Loss: 0.4986, Accuracy: 0.8036\n",
      "Epoch 5/100, Loss: 0.4454, Accuracy: 0.8304\n",
      "Epoch 6/100, Loss: 0.4479, Accuracy: 0.8275\n",
      "Epoch 7/100, Loss: 0.4109, Accuracy: 0.8551\n",
      "Epoch 8/100, Loss: 0.3742, Accuracy: 0.8565\n",
      "Epoch 9/100, Loss: 0.3358, Accuracy: 0.8826\n",
      "Epoch 10/100, Loss: 0.3013, Accuracy: 0.8841\n",
      "Epoch 11/100, Loss: 0.3101, Accuracy: 0.8935\n",
      "Epoch 12/100, Loss: 0.3370, Accuracy: 0.8761\n",
      "Epoch 13/100, Loss: 0.3006, Accuracy: 0.8964\n",
      "Epoch 14/100, Loss: 0.3133, Accuracy: 0.8913\n",
      "Epoch 15/100, Loss: 0.2931, Accuracy: 0.8964\n",
      "Epoch 16/100, Loss: 0.2725, Accuracy: 0.9022\n",
      "Epoch 17/100, Loss: 0.2854, Accuracy: 0.8986\n",
      "Epoch 18/100, Loss: 0.2988, Accuracy: 0.8855\n",
      "Epoch 19/100, Loss: 0.3193, Accuracy: 0.8906\n",
      "Epoch 20/100, Loss: 0.3188, Accuracy: 0.8819\n",
      "Epoch 21/100, Loss: 0.2620, Accuracy: 0.9101\n",
      "Epoch 22/100, Loss: 0.2770, Accuracy: 0.9036\n",
      "Epoch 23/100, Loss: 0.2657, Accuracy: 0.9014\n",
      "Epoch 24/100, Loss: 0.2529, Accuracy: 0.9145\n",
      "Epoch 25/100, Loss: 0.2385, Accuracy: 0.9261\n",
      "Epoch 26/100, Loss: 0.2250, Accuracy: 0.9232\n",
      "Epoch 27/100, Loss: 0.2404, Accuracy: 0.9217\n",
      "Epoch 28/100, Loss: 0.2372, Accuracy: 0.9145\n",
      "Epoch 29/100, Loss: 0.2174, Accuracy: 0.9254\n",
      "Epoch 30/100, Loss: 0.2300, Accuracy: 0.9130\n",
      "Epoch 31/100, Loss: 0.2277, Accuracy: 0.9225\n",
      "Epoch 32/100, Loss: 0.2315, Accuracy: 0.9109\n",
      "Epoch 33/100, Loss: 0.2907, Accuracy: 0.9138\n",
      "Epoch 34/100, Loss: 0.2243, Accuracy: 0.9116\n",
      "Epoch 35/100, Loss: 0.2135, Accuracy: 0.9225\n",
      "Epoch 36/100, Loss: 0.2039, Accuracy: 0.9326\n",
      "Epoch 37/100, Loss: 0.2142, Accuracy: 0.9312\n",
      "Epoch 38/100, Loss: 0.1897, Accuracy: 0.9268\n",
      "Epoch 39/100, Loss: 0.1942, Accuracy: 0.9297\n",
      "Epoch 40/100, Loss: 0.1862, Accuracy: 0.9326\n",
      "Epoch 41/100, Loss: 0.1822, Accuracy: 0.9319\n",
      "Epoch 42/100, Loss: 0.2036, Accuracy: 0.9283\n",
      "Epoch 43/100, Loss: 0.2107, Accuracy: 0.9261\n",
      "Epoch 44/100, Loss: 0.2073, Accuracy: 0.9210\n",
      "Epoch 45/100, Loss: 0.2214, Accuracy: 0.9283\n",
      "Epoch 46/100, Loss: 0.2345, Accuracy: 0.9275\n",
      "Epoch 47/100, Loss: 0.2745, Accuracy: 0.9268\n",
      "Epoch 48/100, Loss: 0.2546, Accuracy: 0.9167\n",
      "Epoch 49/100, Loss: 0.1908, Accuracy: 0.9312\n",
      "Epoch 50/100, Loss: 0.1980, Accuracy: 0.9304\n",
      "Epoch 51/100, Loss: 0.2169, Accuracy: 0.9210\n",
      "Epoch 52/100, Loss: 0.2016, Accuracy: 0.9370\n",
      "Epoch 53/100, Loss: 0.1943, Accuracy: 0.9348\n",
      "Epoch 54/100, Loss: 0.1975, Accuracy: 0.9290\n",
      "Epoch 55/100, Loss: 0.1876, Accuracy: 0.9362\n",
      "Epoch 56/100, Loss: 0.1676, Accuracy: 0.9420\n",
      "Epoch 57/100, Loss: 0.1764, Accuracy: 0.9435\n",
      "Epoch 58/100, Loss: 0.1996, Accuracy: 0.9341\n",
      "Epoch 59/100, Loss: 0.1943, Accuracy: 0.9268\n",
      "Epoch 60/100, Loss: 0.1910, Accuracy: 0.9435\n",
      "Epoch 61/100, Loss: 0.1898, Accuracy: 0.9246\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Funzione per il downsampling\n",
    "def apply_downsampling(dataset, class_counts):\n",
    "    \"\"\"\n",
    "    Riduce il dataset downsampling le classi maggioritarie per bilanciare le proporzioni.\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset completo.\n",
    "        class_counts (dict): Dizionario con la distribuzione delle classi nel formato {classe: conteggio}.\n",
    "    Returns:\n",
    "        Subset: Dataset ridotto.\n",
    "    \"\"\"\n",
    "    min_count = min(class_counts.values())  # Dimensione della classe minoritaria\n",
    "    indices = []\n",
    "    for class_idx, count in class_counts.items():\n",
    "        class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_idx]\n",
    "        downsampled_indices = np.random.choice(class_indices, min_count, replace=False)\n",
    "        indices.extend(downsampled_indices)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Funzione per l'oversampling\n",
    "def apply_oversampling(dataset, class_counts):\n",
    "    \"\"\"\n",
    "    Applica l'oversampling per le classi minoritarie utilizzando WeightedRandomSampler.\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset completo.\n",
    "        class_counts (dict): Dizionario con la distribuzione delle classi nel formato {classe: conteggio}.\n",
    "    Returns:\n",
    "        WeightedRandomSampler: Sampler pesato per DataLoader.\n",
    "    \"\"\"\n",
    "    class_weights = [1.0 / class_counts[label] for _, label in dataset]\n",
    "    sampler = WeightedRandomSampler(weights=class_weights, num_samples=len(dataset), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "# Calcolo della distribuzione delle classi\n",
    "def compute_class_distribution(dataset):\n",
    "    \"\"\"\n",
    "    Calcola la distribuzione delle classi in un dataset.\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset completo.\n",
    "    Returns:\n",
    "        dict: Distribuzione delle classi {classe: conteggio}.\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] = class_counts.get(label, 0) + 1\n",
    "    return class_counts\n",
    "\n",
    "# Applicazione delle strategie\n",
    "class_counts = compute_class_distribution(train_dataset)\n",
    "print(\"Distribuzione originale delle classi:\", class_counts)\n",
    "\n",
    "# Scegli una strategia\n",
    "strategy = \"downsampling\"  # Cambia a \"downsampling\" per valutare il downsampling\n",
    "\n",
    "if strategy == \"downsampling\":\n",
    "    print(\"Applicazione del downsampling...\")\n",
    "    train_dataset = apply_downsampling(train_dataset, class_counts)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "elif strategy == \"oversampling\":\n",
    "    print(\"Applicazione dell'oversampling...\")\n",
    "    sampler = apply_oversampling(train_dataset, class_counts)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "\n",
    "# Rete e allenamento\n",
    "model = CustomResNet50()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Allenamento con la strategia scelta\n",
    "train_net(model, train_loader, optimizer, criterion, epochs=EPOCHS, save_dir=os.path.join(PATH_FOLDER, f'{MODEL}_training_results_{strategy}'))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
