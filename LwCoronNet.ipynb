{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Allenamento in Google Colab di LwCoronNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_F9Dxlkpiz95",
    "outputId": "281d4fd5-12bc-47d2-c9c4-a910424f9acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNxOumNi0tej"
   },
   "outputs": [],
   "source": [
    "PATH_FOLDER = '/content/drive/MyDrive/Colab Notebooks/Pneumonia Detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hx9vwQ4p2h1F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parametri dell'allenamento\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 100\n",
    "OPTIMIZER = 'Adam'\n",
    "LOSS_FUNCTION = 'Categorical Cross-Entropy'\n",
    "H, W = 224, 244\n",
    "\n",
    "# Transformation for Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.9, 1.1)),\n",
    "    transforms.Resize((H, W)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: add_gaussian_noise(x) if torch.rand(1).item() > 0.5 else x),\n",
    "])\n",
    "\n",
    "# Dataset e DataLoader\n",
    "path_to_train = os.path.join(PATH_FOLDER, \"Data\", \"train\")\n",
    "path_to_test = os.path.join(PATH_FOLDER, \"Data\", \"test\")\n",
    "train_dataset = datasets.ImageFolder(path_to_train, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(path_to_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "class LwCoronNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(LwCoronNet, self).__init__()\n",
    "\n",
    "        # Primo blocco convoluzionale\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=2)\n",
    "        self.norm1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(3, stride=3)\n",
    "\n",
    "        # Secondo blocco convoluzionale\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.norm2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(3, stride=3)\n",
    "\n",
    "        # Terzo blocco convoluzionale\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
    "        self.norm3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(3, stride=3)\n",
    "\n",
    "        # Layer completamente connessi\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.norm4 = nn.BatchNorm1d(num_features=2304)\n",
    "        self.drop4 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(in_features=2304, out_features=128)  # Inizializzazione per immagini 224x224\n",
    "\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # Blocchi aggiuntivi di normalizzazione e dropout\n",
    "        self.norm5 = nn.BatchNorm1d(num_features=128)\n",
    "        self.drop5 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passaggio attraverso i blocchi convoluzionali\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Layer completamente connessi\n",
    "        x = self.flatten(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.drop4(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        # Passaggio attraverso i layer aggiuntivi\n",
    "        x = self.norm5(x)\n",
    "        x = self.drop5(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Funzione per aggiungere rumore gaussiano\n",
    "def add_gaussian_noise(img, mean=0, std=0.25):\n",
    "    noise = torch.randn_like(img) * std + mean\n",
    "    noisy_img = img + noise\n",
    "    noisy_img = torch.clamp(noisy_img, 0., 1.)  # Assicuriamoci che i valori siano tra 0 e 1\n",
    "    return noisy_img\n",
    "\n",
    "\n",
    "# Funzione di allenamento\n",
    "def train_net(model, train_loader, optimizer, criterion, epochs, save_dir='./training_results'):\n",
    "    # Creazione della cartella per salvare i modelli e grafici\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['accuracy'].append(epoch_acc)\n",
    "\n",
    "        # Stampa e salvataggio ogni epoca\n",
    "        message = f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\"\n",
    "        print(message)\n",
    "\n",
    "        # Salvataggio del messaggio nel file di log\n",
    "        log_file_path = os.path.join(save_dir, \"training_log.txt\")\n",
    "        save_log_to_file(log_file_path, message)\n",
    "\n",
    "        # Salvataggio del modello ad ogni epoca\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f\"model_epoch_{epoch + 1}.pth\"))\n",
    "\n",
    "        # Salvataggio dei grafici alla fine di ogni epoca\n",
    "        plot_training_curves(history, save_dir, epoch + 1)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def save_log_to_file(file_path, message):\n",
    "    \"\"\"\n",
    "    Salva un messaggio in un file di testo. Crea il file se non esiste.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Il percorso del file in cui salvare il messaggio.\n",
    "        message (str): Il messaggio da salvare.\n",
    "    \"\"\"\n",
    "    # Verifica se il file esiste, altrimenti crealo\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as file:  # 'w' crea il file vuoto\n",
    "            pass  # File creato vuoto, pronto per l'aggiunta di messaggi\n",
    "\n",
    "    # Aggiunge il messaggio al file\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(message + '\\n')\n",
    "\n",
    "\n",
    "# Funzione per i grafici\n",
    "def plot_training_curves(history, save_dir, epoch):\n",
    "    epochs = range(1, len(history['loss']) + 1)  # Epoche da 1 a N\n",
    "\n",
    "    # Creazione dei grafici\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Diagramma della perdita\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['loss'], label='Training Loss', color='red', marker='o')\n",
    "    plt.title('Loss Diagram')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Diagramma dell'accuratezza\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['accuracy'], label='Training Accuracy', color='blue', marker='o')\n",
    "    plt.title('Accuracy Diagram')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Salvataggio dei grafici\n",
    "    plt.savefig(os.path.join(save_dir, f\"training_curves_epoch_{epoch}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpHDWu1Qyf3V",
    "outputId": "f449fe15-fe93-4189-f5fd-7dacb53819e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7160, Accuracy: 0.7234\n",
      "Epoch 2/100, Loss: 0.4571, Accuracy: 0.8542\n",
      "Epoch 3/100, Loss: 0.3550, Accuracy: 0.8871\n",
      "Epoch 4/100, Loss: 0.3108, Accuracy: 0.8946\n",
      "Epoch 5/100, Loss: 0.2894, Accuracy: 0.9014\n",
      "Epoch 6/100, Loss: 0.2694, Accuracy: 0.9114\n",
      "Epoch 7/100, Loss: 0.2468, Accuracy: 0.9121\n",
      "Epoch 8/100, Loss: 0.2274, Accuracy: 0.9182\n",
      "Epoch 9/100, Loss: 0.2175, Accuracy: 0.9230\n",
      "Epoch 10/100, Loss: 0.2114, Accuracy: 0.9285\n",
      "Epoch 11/100, Loss: 0.2006, Accuracy: 0.9304\n",
      "Epoch 12/100, Loss: 0.1952, Accuracy: 0.9318\n",
      "Epoch 13/100, Loss: 0.2023, Accuracy: 0.9347\n",
      "Epoch 14/100, Loss: 0.1862, Accuracy: 0.9360\n",
      "Epoch 15/100, Loss: 0.1866, Accuracy: 0.9360\n",
      "Epoch 16/100, Loss: 0.1817, Accuracy: 0.9357\n",
      "Epoch 17/100, Loss: 0.1774, Accuracy: 0.9392\n"
     ]
    }
   ],
   "source": [
    "# Definizione dell'ottimizzatore e della loss function\n",
    "model = LwCoronNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Allenamento della rete\n",
    "train_net(model, train_loader, optimizer, criterion, epochs=EPOCHS, save_dir=os.path.join(PATH_FOLDER, 'training_results'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MmRuEG7WgzR"
   },
   "source": [
    "# Applicazione tecniche di OverSampling o DownSampling, per dati sbilanciati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWJUjHDJWrDv",
    "outputId": "bca2dee7-122d-4a49-acb4-63d0d6fcbec8"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione originale delle classi: {0: 460, 1: 1266, 2: 3418}\n",
      "Applicazione del downsampling...\n",
      "Epoch 1/100, Loss: 0.7785, Accuracy: 0.6377\n",
      "Epoch 2/100, Loss: 0.6238, Accuracy: 0.7616\n",
      "Epoch 3/100, Loss: 0.5129, Accuracy: 0.8065\n",
      "Epoch 4/100, Loss: 0.4986, Accuracy: 0.8036\n",
      "Epoch 5/100, Loss: 0.4454, Accuracy: 0.8304\n",
      "Epoch 6/100, Loss: 0.4479, Accuracy: 0.8275\n",
      "Epoch 7/100, Loss: 0.4109, Accuracy: 0.8551\n",
      "Epoch 8/100, Loss: 0.3742, Accuracy: 0.8565\n",
      "Epoch 9/100, Loss: 0.3358, Accuracy: 0.8826\n",
      "Epoch 10/100, Loss: 0.3013, Accuracy: 0.8841\n",
      "Epoch 11/100, Loss: 0.3101, Accuracy: 0.8935\n",
      "Epoch 12/100, Loss: 0.3370, Accuracy: 0.8761\n",
      "Epoch 13/100, Loss: 0.3006, Accuracy: 0.8964\n",
      "Epoch 14/100, Loss: 0.3133, Accuracy: 0.8913\n",
      "Epoch 15/100, Loss: 0.2931, Accuracy: 0.8964\n",
      "Epoch 16/100, Loss: 0.2725, Accuracy: 0.9022\n",
      "Epoch 17/100, Loss: 0.2854, Accuracy: 0.8986\n",
      "Epoch 18/100, Loss: 0.2988, Accuracy: 0.8855\n",
      "Epoch 19/100, Loss: 0.3193, Accuracy: 0.8906\n",
      "Epoch 20/100, Loss: 0.3188, Accuracy: 0.8819\n",
      "Epoch 21/100, Loss: 0.2620, Accuracy: 0.9101\n",
      "Epoch 22/100, Loss: 0.2770, Accuracy: 0.9036\n",
      "Epoch 23/100, Loss: 0.2657, Accuracy: 0.9014\n",
      "Epoch 24/100, Loss: 0.2529, Accuracy: 0.9145\n",
      "Epoch 25/100, Loss: 0.2385, Accuracy: 0.9261\n",
      "Epoch 26/100, Loss: 0.2250, Accuracy: 0.9232\n",
      "Epoch 27/100, Loss: 0.2404, Accuracy: 0.9217\n",
      "Epoch 28/100, Loss: 0.2372, Accuracy: 0.9145\n",
      "Epoch 29/100, Loss: 0.2174, Accuracy: 0.9254\n",
      "Epoch 30/100, Loss: 0.2300, Accuracy: 0.9130\n",
      "Epoch 31/100, Loss: 0.2277, Accuracy: 0.9225\n",
      "Epoch 32/100, Loss: 0.2315, Accuracy: 0.9109\n",
      "Epoch 33/100, Loss: 0.2907, Accuracy: 0.9138\n",
      "Epoch 34/100, Loss: 0.2243, Accuracy: 0.9116\n",
      "Epoch 35/100, Loss: 0.2135, Accuracy: 0.9225\n",
      "Epoch 36/100, Loss: 0.2039, Accuracy: 0.9326\n",
      "Epoch 37/100, Loss: 0.2142, Accuracy: 0.9312\n",
      "Epoch 38/100, Loss: 0.1897, Accuracy: 0.9268\n",
      "Epoch 39/100, Loss: 0.1942, Accuracy: 0.9297\n",
      "Epoch 40/100, Loss: 0.1862, Accuracy: 0.9326\n",
      "Epoch 41/100, Loss: 0.1822, Accuracy: 0.9319\n",
      "Epoch 42/100, Loss: 0.2036, Accuracy: 0.9283\n",
      "Epoch 43/100, Loss: 0.2107, Accuracy: 0.9261\n",
      "Epoch 44/100, Loss: 0.2073, Accuracy: 0.9210\n",
      "Epoch 45/100, Loss: 0.2214, Accuracy: 0.9283\n",
      "Epoch 46/100, Loss: 0.2345, Accuracy: 0.9275\n",
      "Epoch 47/100, Loss: 0.2745, Accuracy: 0.9268\n",
      "Epoch 48/100, Loss: 0.2546, Accuracy: 0.9167\n",
      "Epoch 49/100, Loss: 0.1908, Accuracy: 0.9312\n",
      "Epoch 50/100, Loss: 0.1980, Accuracy: 0.9304\n",
      "Epoch 51/100, Loss: 0.2169, Accuracy: 0.9210\n",
      "Epoch 52/100, Loss: 0.2016, Accuracy: 0.9370\n",
      "Epoch 53/100, Loss: 0.1943, Accuracy: 0.9348\n",
      "Epoch 54/100, Loss: 0.1975, Accuracy: 0.9290\n",
      "Epoch 55/100, Loss: 0.1876, Accuracy: 0.9362\n",
      "Epoch 56/100, Loss: 0.1676, Accuracy: 0.9420\n",
      "Epoch 57/100, Loss: 0.1764, Accuracy: 0.9435\n",
      "Epoch 58/100, Loss: 0.1996, Accuracy: 0.9341\n",
      "Epoch 59/100, Loss: 0.1943, Accuracy: 0.9268\n",
      "Epoch 60/100, Loss: 0.1910, Accuracy: 0.9435\n",
      "Epoch 61/100, Loss: 0.1898, Accuracy: 0.9246\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Funzione per il downsampling\n",
    "def apply_downsampling(dataset, class_counts):\n",
    "    \"\"\"\n",
    "    Riduce il dataset downsampling le classi maggioritarie per bilanciare le proporzioni.\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset completo.\n",
    "        class_counts (dict): Dizionario con la distribuzione delle classi nel formato {classe: conteggio}.\n",
    "    Returns:\n",
    "        Subset: Dataset ridotto.\n",
    "    \"\"\"\n",
    "    min_count = min(class_counts.values())  # Dimensione della classe minoritaria\n",
    "    indices = []\n",
    "    for class_idx, count in class_counts.items():\n",
    "        class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_idx]\n",
    "        downsampled_indices = np.random.choice(class_indices, min_count, replace=False)\n",
    "        indices.extend(downsampled_indices)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Funzione per l'oversampling\n",
    "def apply_oversampling(dataset, class_counts):\n",
    "    \"\"\"\n",
    "    Applica l'oversampling per le classi minoritarie utilizzando WeightedRandomSampler.\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset completo.\n",
    "        class_counts (dict): Dizionario con la distribuzione delle classi nel formato {classe: conteggio}.\n",
    "    Returns:\n",
    "        WeightedRandomSampler: Sampler pesato per DataLoader.\n",
    "    \"\"\"\n",
    "    class_weights = [1.0 / class_counts[label] for _, label in dataset]\n",
    "    sampler = WeightedRandomSampler(weights=class_weights, num_samples=len(dataset), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "# Calcolo della distribuzione delle classi\n",
    "def compute_class_distribution(dataset):\n",
    "    \"\"\"\n",
    "    Calcola la distribuzione delle classi in un dataset.\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset completo.\n",
    "    Returns:\n",
    "        dict: Distribuzione delle classi {classe: conteggio}.\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] = class_counts.get(label, 0) + 1\n",
    "    return class_counts\n",
    "\n",
    "# Applicazione delle strategie\n",
    "class_counts = compute_class_distribution(train_dataset)\n",
    "print(\"Distribuzione originale delle classi:\", class_counts)\n",
    "\n",
    "# Scegli una strategia\n",
    "strategy = \"downsampling\"  # Cambia a \"downsampling\" per valutare il downsampling\n",
    "\n",
    "if strategy == \"downsampling\":\n",
    "    print(\"Applicazione del downsampling...\")\n",
    "    train_dataset = apply_downsampling(train_dataset, class_counts)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "elif strategy == \"oversampling\":\n",
    "    print(\"Applicazione dell'oversampling...\")\n",
    "    sampler = apply_oversampling(train_dataset, class_counts)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "\n",
    "# Rete e allenamento\n",
    "model = LwCoronNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Allenamento con la strategia scelta\n",
    "train_net(model, train_loader, optimizer, criterion, epochs=EPOCHS, save_dir=os.path.join(PATH_FOLDER, f'training_results_{strategy}'))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
